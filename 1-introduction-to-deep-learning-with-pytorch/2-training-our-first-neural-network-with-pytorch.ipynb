{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a binary classifier in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6961]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From regression to multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7459, 0.2000, 0.0068, 0.0474]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using loss functions to assess model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes = num_classes)\n",
    "\n",
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0]])\n",
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.Tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "print(one_hot_label)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using derivatives to update model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0229, -0.0306, -0.0458, -0.0535, -0.0764, -0.0917, -0.0153, -0.0229,\n",
      "         -0.0458, -0.0611, -0.0687],\n",
      "        [-0.0270, -0.0361, -0.0541, -0.0631, -0.0901, -0.1082, -0.0180, -0.0270,\n",
      "         -0.0541, -0.0721, -0.0811],\n",
      "        [-0.0041, -0.0055, -0.0083, -0.0097, -0.0138, -0.0166, -0.0028, -0.0041,\n",
      "         -0.0083, -0.0110, -0.0124],\n",
      "        [ 0.0165,  0.0219,  0.0329,  0.0384,  0.0549,  0.0658,  0.0110,  0.0165,\n",
      "          0.0329,  0.0439,  0.0494],\n",
      "        [ 0.0183,  0.0244,  0.0367,  0.0428,  0.0611,  0.0733,  0.0122,  0.0183,\n",
      "          0.0367,  0.0489,  0.0550],\n",
      "        [ 0.0045,  0.0060,  0.0090,  0.0105,  0.0150,  0.0180,  0.0030,  0.0045,\n",
      "          0.0090,  0.0120,  0.0135],\n",
      "        [ 0.0056,  0.0075,  0.0112,  0.0131,  0.0187,  0.0225,  0.0037,  0.0056,\n",
      "          0.0112,  0.0150,  0.0169],\n",
      "        [ 0.0279,  0.0372,  0.0559,  0.0652,  0.0931,  0.1117,  0.0186,  0.0279,\n",
      "          0.0559,  0.0745,  0.0838]])\n",
      "tensor([-0.0076, -0.0090, -0.0014,  0.0055,  0.0061,  0.0015,  0.0019,  0.0093])\n",
      "tensor([[-0.0666, -0.0428, -0.0066, -0.0455,  0.0342,  0.1033,  0.0278,  0.0906],\n",
      "        [-0.0039, -0.0025, -0.0004, -0.0026,  0.0020,  0.0060,  0.0016,  0.0053],\n",
      "        [-0.0573, -0.0369, -0.0056, -0.0391,  0.0294,  0.0889,  0.0239,  0.0780],\n",
      "        [-0.0800, -0.0515, -0.0079, -0.0546,  0.0411,  0.1241,  0.0334,  0.1089],\n",
      "        [ 0.0815,  0.0524,  0.0080,  0.0556, -0.0419, -0.1264, -0.0341, -0.1109],\n",
      "        [-0.0165, -0.0106, -0.0016, -0.0113,  0.0085,  0.0256,  0.0069,  0.0224]])\n",
      "tensor([-0.0115, -0.0007, -0.0099, -0.0139,  0.0141, -0.0029])\n",
      "tensor([[ 0.1570,  0.0202, -0.0185,  0.0638, -0.0491,  0.0231],\n",
      "        [ 0.1233,  0.0159, -0.0145,  0.0501, -0.0386,  0.0181],\n",
      "        [-0.3097, -0.0398,  0.0364, -0.1259,  0.0968, -0.0455],\n",
      "        [ 0.0294,  0.0038, -0.0035,  0.0120, -0.0092,  0.0043]])\n",
      "tensor([ 0.0198,  0.0155, -0.0390,  0.0037])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "target = F.one_hot(torch.tensor([2]), 4).double()\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 8),\n",
    "  nn.Linear(8, 6),\n",
    "  nn.Linear(6, 4)\n",
    ")\n",
    "\n",
    "prediction = model(sample)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "print(model[0].weight.grad)\n",
    "print(model[0].bias.grad)\n",
    "\n",
    "print(model[1].weight.grad)\n",
    "print(model[1].bias.grad)\n",
    "\n",
    "print(model[2].weight.grad) \n",
    "print(model[2].bias.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(16, 8),\n",
    "  nn.Sigmoid(),\n",
    "  nn.Linear(8, 2))\n",
    "\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0971,  0.1001,  0.2619,  0.0493,  0.1219,  0.1040,  0.0658,  0.1791,\n",
      "          0.2696, -0.0118, -0.1172],\n",
      "        [-0.2400, -0.0057, -0.2672,  0.2262,  0.1076,  0.2741,  0.0531,  0.1012,\n",
      "          0.0129,  0.0062, -0.1481],\n",
      "        [ 0.2260, -0.1034, -0.1302, -0.2156, -0.1575,  0.0519, -0.2090, -0.2779,\n",
      "          0.0301, -0.2071, -0.0193],\n",
      "        [ 0.1437, -0.0198,  0.2390, -0.1248,  0.1819, -0.2085, -0.0973,  0.2100,\n",
      "          0.0214, -0.0627, -0.1358],\n",
      "        [-0.0306, -0.1658, -0.1572,  0.0218,  0.1323, -0.2719,  0.2263, -0.1176,\n",
      "         -0.1964,  0.1460, -0.0273],\n",
      "        [ 0.0725,  0.1948, -0.0827,  0.1041, -0.0362,  0.1914, -0.1233, -0.1045,\n",
      "         -0.0502, -0.1455, -0.0590],\n",
      "        [-0.1938, -0.2939, -0.2506,  0.1835, -0.1931, -0.2612, -0.1550,  0.1672,\n",
      "          0.2538,  0.1801,  0.2805],\n",
      "        [-0.1130, -0.2596,  0.1447,  0.0288,  0.1625,  0.0783, -0.2831, -0.0605,\n",
      "         -0.0363, -0.1258, -0.1563]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 0.2667, -0.2727,  0.0991,  0.2548, -0.1522, -0.0920, -0.1105,  0.0197],\n",
      "        [ 0.1546,  0.3103, -0.1270, -0.0600, -0.1440,  0.3081, -0.2145,  0.1710],\n",
      "        [-0.1929,  0.0206, -0.0618, -0.1639, -0.2880,  0.0597,  0.0585, -0.0586],\n",
      "        [ 0.0509, -0.1684,  0.1184, -0.1987, -0.3350,  0.3135, -0.0533,  0.3289],\n",
      "        [ 0.0201, -0.2645, -0.2611,  0.1752,  0.3458, -0.2085,  0.2138, -0.2920],\n",
      "        [ 0.2129,  0.2138,  0.2249,  0.0213,  0.2434, -0.2447,  0.2372,  0.0686]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([[-0.3651,  0.3087,  0.3522,  0.2297,  0.2220,  0.3797],\n",
      "        [-0.1920,  0.2449,  0.3045, -0.3443,  0.1267,  0.1219],\n",
      "        [-0.2804,  0.2996,  0.2897, -0.4062,  0.3429,  0.3646],\n",
      "        [-0.2090,  0.2458,  0.1696,  0.0614,  0.3859, -0.3649]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "target = F.one_hot(torch.tensor([2]), 4).double()\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 8),\n",
    "  nn.Linear(8, 6),\n",
    "  nn.Linear(6, 4)\n",
    ")\n",
    "\n",
    "prediction = model(sample)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2\n",
    "\n",
    "print(weight0)\n",
    "print(weight1)\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PyTorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing your first training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2294],\n",
      "        [-0.3355],\n",
      "        [-0.3355]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7500],\n",
      "        [0.1099],\n",
      "        [0.1636]])\n"
     ]
    }
   ],
   "source": [
    "features = [[1., 0., 1., 1.], [1., 0., 0., 1.], [1., 0., 1., 1.]]\n",
    "target = [[0.1099], [0.7500], [0.1636]]\n",
    "\n",
    "dataset = TensorDataset(torch.Tensor(features).float(), torch.Tensor(target).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(4, 2),\n",
    "  nn.Linear(2, 1))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 8\n",
    "\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    loss = criterion(prediction, target)    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "prediction = model(feature)  \n",
    "print(prediction) \n",
    "print(target)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-in-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
