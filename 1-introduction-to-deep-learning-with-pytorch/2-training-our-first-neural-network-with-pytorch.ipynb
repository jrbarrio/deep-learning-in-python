{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a binary classifier in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1185]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From regression to multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0673, 0.0211, 0.4520, 0.4596]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using loss functions to assess model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes = num_classes)\n",
    "\n",
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0]])\n",
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.Tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "print(one_hot_label)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using derivatives to update model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0544, -0.0725, -0.1088, -0.1269, -0.1814, -0.2176, -0.0363, -0.0544,\n",
      "         -0.1088, -0.1451, -0.1632],\n",
      "        [ 0.0600,  0.0799,  0.1199,  0.1399,  0.1999,  0.2398,  0.0400,  0.0600,\n",
      "          0.1199,  0.1599,  0.1799],\n",
      "        [ 0.0817,  0.1089,  0.1633,  0.1906,  0.2722,  0.3267,  0.0544,  0.0817,\n",
      "          0.1633,  0.2178,  0.2450],\n",
      "        [-0.0058, -0.0077, -0.0116, -0.0135, -0.0193, -0.0232, -0.0039, -0.0058,\n",
      "         -0.0116, -0.0155, -0.0174],\n",
      "        [ 0.0068,  0.0091,  0.0136,  0.0159,  0.0227,  0.0272,  0.0045,  0.0068,\n",
      "          0.0136,  0.0182,  0.0204],\n",
      "        [ 0.0260,  0.0346,  0.0519,  0.0606,  0.0866,  0.1039,  0.0173,  0.0260,\n",
      "          0.0519,  0.0693,  0.0779],\n",
      "        [ 0.0319,  0.0425,  0.0638,  0.0744,  0.1064,  0.1276,  0.0213,  0.0319,\n",
      "          0.0638,  0.0851,  0.0957],\n",
      "        [-0.0442, -0.0590, -0.0885, -0.1032, -0.1475, -0.1770, -0.0295, -0.0442,\n",
      "         -0.0885, -0.1180, -0.1327]])\n",
      "tensor([-0.0181,  0.0200,  0.0272, -0.0019,  0.0023,  0.0087,  0.0106, -0.0147])\n",
      "tensor([[ 0.0697, -0.1340, -0.0135,  0.0380,  0.0916,  0.0752, -0.2692,  0.1744],\n",
      "        [-0.0411,  0.0789,  0.0079, -0.0224, -0.0540, -0.0443,  0.1586, -0.1027],\n",
      "        [ 0.0523, -0.1005, -0.0101,  0.0285,  0.0688,  0.0564, -0.2020,  0.1309],\n",
      "        [-0.0529,  0.1015,  0.0102, -0.0288, -0.0694, -0.0570,  0.2040, -0.1322],\n",
      "        [-0.0500,  0.0960,  0.0096, -0.0272, -0.0656, -0.0538,  0.1928, -0.1249],\n",
      "        [-0.0026,  0.0050,  0.0005, -0.0014, -0.0035, -0.0028,  0.0101, -0.0066]])\n",
      "tensor([ 0.0347, -0.0204,  0.0260, -0.0263, -0.0248, -0.0013])\n",
      "tensor([[-1.3975e-01,  4.7677e-02, -7.3900e-02, -4.6362e-04,  1.2117e-01,\n",
      "          9.0606e-02],\n",
      "        [-7.3936e-02,  2.5224e-02, -3.9098e-02, -2.4528e-04,  6.4105e-02,\n",
      "          4.7936e-02],\n",
      "        [ 2.5436e-01, -8.6777e-02,  1.3451e-01,  8.4383e-04, -2.2054e-01,\n",
      "         -1.6491e-01],\n",
      "        [-4.0673e-02,  1.3876e-02, -2.1508e-02, -1.3493e-04,  3.5265e-02,\n",
      "          2.6370e-02]])\n",
      "tensor([ 0.0427,  0.0226, -0.0777,  0.0124])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "target = F.one_hot(torch.tensor([2]), 4).double()\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 8),\n",
    "  nn.Linear(8, 6),\n",
    "  nn.Linear(6, 4)\n",
    ")\n",
    "\n",
    "prediction = model(sample)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "print(model[0].weight.grad)\n",
    "print(model[0].bias.grad)\n",
    "\n",
    "print(model[1].weight.grad)\n",
    "print(model[1].bias.grad)\n",
    "\n",
    "print(model[2].weight.grad) \n",
    "print(model[2].bias.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(16, 8),\n",
    "  nn.Sigmoid(),\n",
    "  nn.Linear(8, 2))\n",
    "\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1142, -0.2652,  0.0361, -0.2031, -0.1615,  0.0458,  0.2113,  0.1753,\n",
      "          0.0666,  0.0026,  0.0182],\n",
      "        [ 0.0633,  0.0761,  0.2609, -0.0135,  0.2406,  0.1823, -0.1064, -0.2179,\n",
      "         -0.0384,  0.0174, -0.0534],\n",
      "        [ 0.0685,  0.0482,  0.2956,  0.0875,  0.2571, -0.2901, -0.0320, -0.2858,\n",
      "          0.2202, -0.1376,  0.0757],\n",
      "        [ 0.0025,  0.1906, -0.0160, -0.0039, -0.2458,  0.1176,  0.0619, -0.1256,\n",
      "         -0.0352,  0.1972,  0.0676],\n",
      "        [ 0.0610,  0.1847,  0.0978, -0.2638,  0.0666,  0.1897,  0.1581, -0.2218,\n",
      "          0.1493, -0.0674, -0.1452],\n",
      "        [-0.1145,  0.2745,  0.2853, -0.1747, -0.1758, -0.0903,  0.2731, -0.0406,\n",
      "          0.0194,  0.0793, -0.2222],\n",
      "        [-0.1546,  0.1802,  0.0311, -0.1750, -0.1461, -0.2675, -0.2764,  0.0827,\n",
      "         -0.0815, -0.2837, -0.2068],\n",
      "        [ 0.1601, -0.2550,  0.1258, -0.2115,  0.1603, -0.0755,  0.1136, -0.2030,\n",
      "         -0.0019, -0.0849,  0.2582]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 0.0997,  0.1416, -0.3024, -0.1658,  0.3319, -0.1301,  0.0931, -0.3048],\n",
      "        [ 0.2475,  0.0303, -0.0423, -0.2244, -0.0030, -0.1822,  0.2848, -0.0194],\n",
      "        [ 0.1477, -0.0259, -0.1664,  0.0450,  0.0449,  0.0595, -0.3522, -0.2981],\n",
      "        [-0.1664, -0.1436, -0.0264,  0.3211, -0.2327,  0.0098, -0.1427, -0.0306],\n",
      "        [ 0.0929,  0.2252,  0.2416, -0.0600, -0.1286, -0.2713, -0.0721, -0.2145],\n",
      "        [-0.2551, -0.2142,  0.3055, -0.1442,  0.0505, -0.0882, -0.0717, -0.0525]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([[-0.1564, -0.1142, -0.1952,  0.1738, -0.0985,  0.2924],\n",
      "        [ 0.1773, -0.2070, -0.3904,  0.1305,  0.1651, -0.3846],\n",
      "        [-0.0791,  0.1127,  0.1750,  0.0605,  0.0838, -0.2796],\n",
      "        [ 0.3633, -0.2673, -0.3486,  0.2780,  0.1822,  0.1007]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "target = F.one_hot(torch.tensor([2]), 4).double()\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 8),\n",
    "  nn.Linear(8, 6),\n",
    "  nn.Linear(6, 4)\n",
    ")\n",
    "\n",
    "prediction = model(sample)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2\n",
    "\n",
    "print(weight0)\n",
    "print(weight1)\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PyTorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-in-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
